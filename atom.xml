<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Han, Lanyi&#39;s Personal Blog</title>
  
  
  <link href="https://hanlanyi.github.io/atom.xml" rel="self"/>
  
  <link href="https://hanlanyi.github.io/"/>
  <updated>2025-07-30T17:57:30.142Z</updated>
  <id>https://hanlanyi.github.io/</id>
  
  <author>
    <name>HAN, Lanyi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AI AR Assistant</title>
    <link href="https://hanlanyi.github.io/2025/06/11/2506-AI-AR-Assistant/"/>
    <id>https://hanlanyi.github.io/2025/06/11/2506-AI-AR-Assistant/</id>
    <published>2025-06-11T17:50:02.000Z</published>
    <updated>2025-07-30T17:57:30.142Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AiArAssistant"><a href="#AiArAssistant" class="headerlink" title="AiArAssistant"></a><a href="https://github.com/hanlanyi/AiArAssistant">AiArAssistant</a></h1><p><img src="/pic/2506Overview.png" alt="System Overview"></p><p><strong>AiArAssistant</strong> is a next-generation software framework that fuses <strong>Artificial Intelligence (AI)</strong> with <strong>Augmented Reality (AR)</strong> to create intelligent, interactive, and dynamic AR experiences.</p><p>This project empowers AR environments with advanced AI capabilities—ranging from contextual information retrieval to autonomous manipulation of virtual objects—by seamlessly integrating local Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and the MCP (Model Context Protocol) servers that intelligently routes user intents through embedded agents, all while maintaining real-time performance across AR platforms.</p><h2 id="✨-Key-Features"><a href="#✨-Key-Features" class="headerlink" title="✨ Key Features"></a>✨ Key Features</h2><ul><li><p><strong>Intelligent AR Interaction</strong><br>Control and modify virtual 3D objects in AR using natural language commands.</p></li><li><p><strong>MCP Server with Agent Routing</strong><br>The <strong>MCP (Model Context Protocol) server</strong> houses the Agent system to interpret, classify, and route user intents to the most suitable AI modules—enabling seamless orchestration of tasks and workflows.</p></li><li><p><strong>RAG-Enhanced Knowledge</strong><br>Retrieve and generate contextually accurate responses from personal or domain-specific knowledge bases using Retrieval-Augmented Generation.</p></li><li><p><strong>Modular MLOps + LoRA</strong><br>Fine-tune and manage models locally with Hugging Face, GPU acceleration, and modular components for training, embedding, and ranking.</p></li><li><p><strong>Cross-Device AR Compatibility</strong><br>Works with a variety of AR glasses and devices, enabling portability and scalability.</p></li></ul><h2 id="🧠-Architecture-Overview"><a href="#🧠-Architecture-Overview" class="headerlink" title="🧠 Architecture Overview"></a>🧠 Architecture Overview</h2><p>The system is divided into two main domains:</p><h3 id="AI-Modules"><a href="#AI-Modules" class="headerlink" title="AI Modules"></a>AI Modules</h3><ul><li><strong>MLOps</strong>: Handles training, validation, and fine-tuning (e.g., LoRA) of models.</li><li><strong>Local LLM</strong>: Runs inference using on-device LLMs powered by Hugging Face and shared memory for speed.</li><li><strong>RAG</strong>: Retrieves knowledge from documents and combines it with LLMs for grounded responses.</li><li><strong>MCP Server</strong>: A central coordination hub that includes:<ul><li><strong>Agent</strong>: Classifies user intents and selects the proper tools or modules to handle them.</li></ul></li></ul><h3 id="AR-Modules"><a href="#AR-Modules" class="headerlink" title="AR Modules"></a>AR Modules</h3><ul><li><strong>AR Network</strong>: Connects AR devices like headsets and smart glasses.</li><li><strong>ARO Manager</strong>: Controls AR objects and manages their properties and behaviors.</li><li><strong>AI Client</strong>: Communicates with the AI backend (including the MCP server), sends and receives commands.</li><li><strong>UI</strong>: User interface for inputting commands and visualizing the AR environment.</li></ul><hr><blockquote><p>🧩 <strong>AiArAssistant</strong> aims to bridge the gap between immersive environments and intelligent systems—giving users the power to not only explore AR, but to shape it with their words via the <strong>MCP server</strong>.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AiArAssistant&quot;&gt;&lt;a href=&quot;#AiArAssistant&quot; class=&quot;headerlink&quot; title=&quot;AiArAssistant&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/hanlanyi/AiArAssist</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>INMO Air Development Guide</title>
    <link href="https://hanlanyi.github.io/2024/06/07/2406-INMO-Air-Development-Guide/"/>
    <id>https://hanlanyi.github.io/2024/06/07/2406-INMO-Air-Development-Guide/</id>
    <published>2024-06-08T01:06:11.000Z</published>
    <updated>2024-06-08T01:33:26.729Z</updated>
    
    <content type="html"><![CDATA[<p>Hi there! I’d love to hear your thoughts. Feel free to leave a <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">comment</a>, even if it’s just a simple ‘Hi’.</p><h1 id="1-INMO-Air-ADB-Activation"><a href="#1-INMO-Air-ADB-Activation" class="headerlink" title="1. INMO Air ADB Activation"></a>1. INMO Air ADB Activation</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Android Debug Bridge (ADB) is a versatile command-line tool that allows you to communicate with a device. ADB commands help with various device operations, such as installing and debugging applications.</p><p>Warning: Due to the openness of the Android system, using ADB to install third-party apps and perform any operations on the system’s native settings may lead to device lag, overheating, and bugs. We will not be responsible for these issues. Once ADB is enabled, the INMO Air2 will not support returns without reason within seven days.</p><h2 id="2-Initial-Preparation-for-the-Glasses"><a href="#2-Initial-Preparation-for-the-Glasses" class="headerlink" title="2. Initial Preparation for the Glasses"></a>2. Initial Preparation for the Glasses</h2><p>On this interface, long press the right touchpad twice, each time for 1.5 seconds, and enter the password 20210108 to enable the ADB mode on the glasses.</p><p><img src="/pic/2406INMO.png" alt="INMO Air Setting"></p><h1 id="2-Unity-SDK"><a href="#2-Unity-SDK" class="headerlink" title="2. Unity SDK"></a>2. Unity SDK</h1><h2 id="SDK-Function-Introduction"><a href="#SDK-Function-Introduction" class="headerlink" title="SDK Function Introduction"></a>SDK Function Introduction</h2><p>This section provides a development guide for the INMO AIR2 SDK on the Unity platform. It includes information on how to adapt development for INMO RING2 on INMO Air2, touchpad interaction development for INMO Air2 AR glasses, and AR capability development kits on the Unity platform.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Download-the-SDK"><a href="#Download-the-SDK" class="headerlink" title="Download the SDK"></a>Download the SDK</h3><p><a href="https://github.com/hanlanyi/INMO-Unity-SDK">INMO SDK</a></p><h3 id="Environment-Configuration"><a href="#Environment-Configuration" class="headerlink" title="Environment Configuration"></a>Environment Configuration</h3><h4 id="Software-Environment"><a href="#Software-Environment" class="headerlink" title="Software Environment"></a>Software Environment</h4><ul><li>Unity Hub and Unity 2020.3LTS or higher, with the Android Support module installed.</li><li>Visual Studio 2019.</li></ul><p>To install Unity Hub and Unity Editor, please visit the <a href="https://unity.com/">Unity Official Website</a>.</p><p><strong>How to install Android Support?</strong></p><ol><li>Open Unity Hub, click on the installations on the left, select the version that needs Android Support, and click the gear icon.</li><li>Click Add Modules, find Android Build Support, and click Install. After installation, the Android label will appear under that version.</li></ol><p><strong>Recommended installation of Visual Studio 2019 through the above add modules method.</strong></p><p><strong>Correspondence between Android NDK and Unity versions:</strong></p><table><thead><tr><th>Unity Version</th><th>NDK Version</th></tr></thead><tbody><tr><td>2020.3 LTS</td><td>r19</td></tr><tr><td>2021.3 LTS</td><td>r21d</td></tr></tbody></table><h4 id="Hardware-Environment"><a href="#Hardware-Environment" class="headerlink" title="Hardware Environment"></a>Hardware Environment</h4><ul><li>INMO AIR2 Glasses</li><li>INMO RING2 Ring</li></ul><h3 id="Import-the-SDK"><a href="#Import-the-SDK" class="headerlink" title="Import the SDK"></a>Import the SDK</h3><p>The <code>inmo_unity_sdk</code> package structure is as follows:<br><img src="/pic/2406_INMO_import_unity.png" alt="INMO Unity SDK Structure"></p><ul><li><code>Samples</code>: Examples provided by the SDK.</li><li><code>SDK</code>: AAR library for AR services.</li><li><code>URP</code>: Pipeline configuration. If your project already has URP configuration or uses the Built-in pipeline, you can delete the URP folder.</li><li><code>Document</code>: Offline documentation.</li><li><code>InmoUnitySdk</code>: SDK assembly.</li></ul><p><strong>Import Guide:</strong></p><ol><li>Create a new project or open an existing project and switch to the Android platform via <code>File -&gt; Build Settings</code>.<br><img src="/pic/2406_INMO_unity_build_android.png" alt="Unity Android Building"></li><li>Download the SDK sample code and import it into the project, or import the latest <code>unitypackage</code> provided by the SDK:<ul><li>Drag the <code>unitypackage</code> directly into the Project window.</li><li>Through <code>Assets -&gt; Import Package -&gt; Custom Package</code>, click the import button at the bottom right of the page and wait for the compilation to complete.</li></ul></li></ol><h3 id="Packaging-Settings"><a href="#Packaging-Settings" class="headerlink" title="Packaging Settings"></a>Packaging Settings</h3><p>Since AR services are developed based on <code>android-28</code>, you need to configure the Minimum API Level to 28 during packaging. The default Scripting Backend is Mono mode, but Il2Cpp mode is recommended for better performance and scalability. The SDK uses the ARMv7 architecture, so you can uncheck ARM64 in Target Architectures to reduce package size. It is recommended to uncheck the “Optimize Frame Rate” option when packaging, as shown below:</p><p><img src="/pic/2406_INMO_unity_build_setting1.png" alt="Unity Project Setting 1"></p><p><img src="/pic/2406_INMO_unity_build_setting2.png" alt="Unity Project Setting 2"></p><h3 id="3D-Model-Resource-Requirements"><a href="#3D-Model-Resource-Requirements" class="headerlink" title="3D Model Resource Requirements"></a>3D Model Resource Requirements</h3><h4 id="Recommended-Rendering-Settings-for-INMO-AIR2-Unity-Development"><a href="#Recommended-Rendering-Settings-for-INMO-AIR2-Unity-Development" class="headerlink" title="Recommended Rendering Settings for INMO AIR2 Unity Development"></a>Recommended Rendering Settings for INMO AIR2 Unity Development</h4><ul><li>The tested simultaneous rendering is about 100,000 to 200,000 faces. Models need to be lightweight, reducing the number of faces as much as possible.</li><li>Use only one Directional Light as a dynamic light source; use lightmaps for multiple lights.</li><li>Set the Camera’s Render Shadows to off, and the Directional Light’s Shadow Type to No Shadows.</li><li>Set the Camera’s Field Of View to 13.4, Near to 0.1, and Far to 15.</li><li>Avoid using low-performance shaders, such as Highlight outlines for the entire model, which will double the number of faces rendered.</li></ul><h1 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h1><ol><li><a href="https://x0kcf6iksn.feishu.cn/docx/R7q9dxHdao5H0FxjdgScLFKxn3v">INMO Air ADB Activation</a></li><li><a href="https://gitee.com/inmolens/docs#unity-sdk">Unity SDK</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hi there! I’d love to hear your thoughts. Feel free to leave a &lt;a href=&quot;https://github.com/hanlanyi/hanlanyi.github.io/issues&quot;&gt;comment&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://hanlanyi.github.io/2024/05/12/2405-hello-world/"/>
    <id>https://hanlanyi.github.io/2024/05/12/2405-hello-world/</id>
    <published>2024-05-13T00:06:03.435Z</published>
    <updated>2024-07-22T14:24:52.319Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to my personal website.</p><p>Hi there! I’d love to hear your thoughts. Feel free to leave a <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">comment</a>, even if it’s just a simple ‘Hi’.</p><h3 id="Sites"><a href="#Sites" class="headerlink" title="Sites"></a>Sites</h3><p><a href="https://www.youtube.com/@hanlanyi">YouTube</a></p><p><a href="https://www.reddit.com/user/hanlanyi/">Reddit</a></p><p><a href="https://x.com/hanlanyiaa">X</a></p><p><a href="https://www.patreon.com/hanlanyi">Patreon</a></p><p><a href="https://www.instagram.com/hanlanyi">Instagram</a></p><p><a href="https://medium.com/@hanlanyi">Medium</a></p><p><a href="https://github.com/hanlanyi">GitHub</a></p><p><a href="https://hanlanyi.itch.io/">itch</a></p><p><a href="https://tiktok.com/@hanlanyi">tiktok</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to my personal website.&lt;/p&gt;
&lt;p&gt;Hi there! I’d love to hear your thoughts. Feel free to leave a &lt;a href=&quot;https://github.com/hanlany</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>An Ultimate Guide of Using Python in Unity (Part I)</title>
    <link href="https://hanlanyi.github.io/2024/05/10/2405-An-Ultimate-Guide-of-Using-Python-in-Unity-Part-I/"/>
    <id>https://hanlanyi.github.io/2024/05/10/2405-An-Ultimate-Guide-of-Using-Python-in-Unity-Part-I/</id>
    <published>2024-05-11T00:29:05.000Z</published>
    <updated>2024-06-08T01:33:48.161Z</updated>
    
    <content type="html"><![CDATA[<p>Hi there! I’d love to hear your thoughts. Feel free to leave a <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">comment</a>, even if it’s just a simple ‘Hi’.</p><p>Python is the most popular programming language in the world. Unity is the most popular game engine in the world. To combine these two, a user guide from Unity is all you need. A very noticable point is that the document of version 7 is way more abundant than all the ones of previous versions combined.</p><p><a href="https://docs.unity3d.com/Packages/com.unity.scripting.python@7.0/manual/index.html">https://docs.unity3d.com/Packages/com.unity.scripting.python@7.0/manual/index.html</a></p><p>But, to work on a much harder and much more complex project, You need my guide.</p><h2 id="1-Python-Scripting-Manager-Version"><a href="#1-Python-Scripting-Manager-Version" class="headerlink" title="1. Python Scripting Manager Version"></a>1. Python Scripting Manager Version</h2><p>The latest version of the Unity Python Scripting manager is 7.0.1; however, by searching “python” in the package manager, the extension I got was 6.0.1, which stopped updating since 2022–08–04. Since 2023–03–21, Unity release 7.0 version and updated to 7.0.1 with more features and less bugs.</p><p>If you had the same issue I had, simply go to “Package Manager” -&gt; “+” -&gt; “Add Package from git URL”, then type “<a href="mailto:&#x63;&#111;&#109;&#x2e;&#x75;&#110;&#105;&#x74;&#121;&#46;&#x73;&#99;&#x72;&#105;&#112;&#116;&#x69;&#x6e;&#103;&#46;&#x70;&#x79;&#x74;&#x68;&#x6f;&#110;&#x40;&#x37;&#x2e;&#48;&#46;&#x31;">&#x63;&#111;&#109;&#x2e;&#x75;&#110;&#105;&#x74;&#121;&#46;&#x73;&#99;&#x72;&#105;&#112;&#116;&#x69;&#x6e;&#103;&#46;&#x70;&#x79;&#x74;&#x68;&#x6f;&#110;&#x40;&#x37;&#x2e;&#48;&#46;&#x31;</a>“. Done.</p><h2 id="2-For-Anaconda-Users"><a href="#2-For-Anaconda-Users" class="headerlink" title="2. For Anaconda Users"></a>2. For Anaconda Users</h2><p>All the python scripts are running in a separated Unity project, which makes a very neat python coding environment without worrying about conda or venv. However, if you are a conda user like I am, please notice whether the terminal in “Project Manager” -&gt; “Python Scripting” triggers conda virtual environment automatically.</p><p>If it does, all the pip installation will be located in the conda location. To prevent it happens, make sure deactivate the virtual env. Then, all the installations will be successfully installed in Unity’s “Library&#x2F;PythonInstall&#x2F;Lib&#x2F;site-packages”.</p><h2 id="3-Run-a-Juypter-Notebook"><a href="#3-Run-a-Juypter-Notebook" class="headerlink" title="3. Run a Juypter Notebook"></a>3. Run a Juypter Notebook</h2><p>Jupyter Notebook is my favorite brainstorming and demo testing platform, due its instant interaction and lego like code structuring layouts. Although python scripts are mostly short and easy to debug, Jupyter Notebook boosts the process even more efficient.</p><p>Install Jupyter Notebook server：</p><p><a href="https://jupyter.org/install">Project Jupyter | Installing Jupyter</a></p><p>Jupyter Lab is recommended, or every single click opens a new tap sounds reasonable, then jupyter notebook can also be tested, although people will always choose back to Jupyter Lab.</p><p>Once it is installed, a simple “jupyter lab” on Unity python script terminal will open a web page instantly, which is the exact location of the Unity Project. All the folders and files on the left side will look familiar.</p><p>Alternatively, there is another way of using Jupyter. I personally use vscode for everything, from machine learning to Unity, even writing with the vim extension. VSCode has such a abundant extension libraries, which greatly has one for Jupyter.</p><p>Once the extension is installed, click the top right to select a kernal.</p><p>Make sure jupyter lab is still running. The extension asks the server, password, and the port. The password is the token from terminal which runs jupyter backend.</p><p>In my case is</p><p>38368ba9ca2fb4fac974672e1c0e37419d10c4baf0c1c69e</p><p>Which is the number right after “<a href="http://localhost:8888/lab?token=">http://localhost:8888/lab?token=</a>“</p><p>Create a file with .ipynb format, for example “test.ipynb”. Now you can enjoy my favorite python brainstorming and demo testing platform in Unity.</p><p>More is on the way.</p><p>If you have any comments please share it in the <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">issues</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hi there! I’d love to hear your thoughts. Feel free to leave a &lt;a href=&quot;https://github.com/hanlanyi/hanlanyi.github.io/issues&quot;&gt;comment&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>A Prototype Document of a LLM Running on an AR Device</title>
    <link href="https://hanlanyi.github.io/2024/04/28/2404-A-Prototype-Document-of-a-LLM-Running-on-an-AR-Device/"/>
    <id>https://hanlanyi.github.io/2024/04/28/2404-A-Prototype-Document-of-a-LLM-Running-on-an-AR-Device/</id>
    <published>2024-04-28T10:33:22.000Z</published>
    <updated>2024-06-08T01:33:34.574Z</updated>
    
    <content type="html"><![CDATA[<p>Hi there! I’d love to hear your thoughts. Feel free to leave a <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">comment</a>, even if it’s just a simple ‘Hi’.</p><p>A Prototype Document of a LLM Running on an AR Device</p><p>Meta Smart Glasses do not have screens that makes the voice commanding the only option for the users. For me, typing and reading is a way more convenient way than speaking and listening.</p><p>Having an AI assistant on a VR or an MR device is clearly not a wise choice either; not only due to the neck killing weight that most people cannot hold for more than an hour, but also the light speed like battery drainning ratio that saves the users’ necks being broke.</p><p>The AR glasses became the best choice among all technological devices, which obviously cannot be a smart phone that staring at one too long will only causes hemiplegia.</p><p>More importantly, I got a pair of AR glasses.</p><p>And here is the plan that I got:</p><p>I will create an app that calls API from the existing LLMs, which can be a ChatGPT or a home server running llama, and the best part is that the chatting UI will not disappear from your sight unless you take off the glasses.</p><p>There will be a gigantic button on the middle of the phone screen that would start the users chatting without moving the eye sight from the AR glasses. Once the button is pressed, the keyboard pops up. Since now days everyone is able to type on the smartphone without looking at it, which also helps users keep their neck releases pressure. No one still needs to look at the phone keyboard to type, right?</p><p>Then the AI assistant will hit the answer right in front of the user, elegant. To satisfy the other group of users who prefer speaking and listening over typing and reading, there will be another button that would triger this specific need. The text of the answer will also be typed on the screen, to make sure people wont miss out certain extremely important information while the AI shouts out an ancient people don’t familar with.</p><p>Furthermore, a picture input with a phone camera will also be supported. People use AR glasses only because they do not want to be segragated from the reality, and the camera is the only way the device can communicate with the information from the reality that carried by photons. Which means, there will be another button to activate the camera.</p><p>So,</p><p>There will be two parts of the app. One is the information gaining interface, which would be the screen on the AR glasses. The other one is the input interface, which would be the phone. The AR glasses will show the info below:</p><ol><li>Typing process animation with text.</li><li>Final text typed in.</li><li>Result text.</li><li>Listening animation.</li><li>Camera capture in realtime.</li><li>Final camera capture.</li><li>A cartoon figure follows you all the time like a Microsoft Office Clippy.</li></ol><p>The phone will do:</p><ol><li>A button activate the keyboard.</li><li>A button activate the microphone.</li><li>A button activate the camera.</li><li>A button calibrate the axis.</li><li>A button exits.</li></ol><p>From now on, I will start working on this project. Hopefully I can demostrate it to the public in my lifetime if my procastination does not drag my feet. One mothed to prevent it happens is that I write blogs more frequently, and that is what I am going to do.</p><p>If you have any comments please share it in the <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">issues</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hi there! I’d love to hear your thoughts. Feel free to leave a &lt;a href=&quot;https://github.com/hanlanyi/hanlanyi.github.io/issues&quot;&gt;comment&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>XReal Air: Transforming Reality in Three Unique Ways</title>
    <link href="https://hanlanyi.github.io/2024/04/18/2404-XReal-Air-Transforming-Reality-in-Three-Unique-Ways/"/>
    <id>https://hanlanyi.github.io/2024/04/18/2404-XReal-Air-Transforming-Reality-in-Three-Unique-Ways/</id>
    <published>2024-04-18T10:35:32.000Z</published>
    <updated>2024-06-08T01:33:42.052Z</updated>
    
    <content type="html"><![CDATA[<p>Hi there! I’d love to hear your thoughts. Feel free to leave a <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">comment</a>, even if it’s just a simple ‘Hi’.</p><p>The XReal Air stands as my all-time favorite augmented reality (AR) device, cause it is the only one that I have. Just kidding. The real reason is that over a year of exploration, I’ve identified three optimal scenarios for deploying this innovative technology, which makes it stand out from all other products.</p><h2 id="1-Partnered-with-a-Smartphone-Featuring-a-Fully-Functioning-Type-C-Port"><a href="#1-Partnered-with-a-Smartphone-Featuring-a-Fully-Functioning-Type-C-Port" class="headerlink" title="1. Partnered with a Smartphone Featuring a Fully Functioning Type C Port"></a>1. Partnered with a Smartphone Featuring a Fully Functioning Type C Port</h2><p>Any smartphone equipped with a fully functional Type C port can seamlessly connect to the XReal Air. Upon connection, users can engage with two distinct modes: mirroring and AR space.</p><p>Mirroring essentially allows you to view your phone’s display through the glasses. While not overly complex, this feature significantly reduces neck strain caused by constantly looking down at your phone. Buy it for everyone must save tons of medical insurance budget.</p><p>In contrast, AR space offers an immersive experience by projecting a curved screen around you. This mode isn’t just for watching YouTube videos or web browsing; it also supports the download of VR apps, unlocking smartphone experiences previously unimaginable. My personal favorite, a maze VR app, dazzles with its full 3D environment and interesting gameplay, moving the platform that drops the ball into the goal.</p><p>This setup excels in environments where mobility is key. Whether lounging on a couch (where it’s practically impossible to maintain a single position) or moving around the house, the XReal Air paired with a smartphone affords unparalleled freedom.</p><h2 id="2-Connected-to-a-Computer-with-a-Type-C-DP-Port"><a href="#2-Connected-to-a-Computer-with-a-Type-C-DP-Port" class="headerlink" title="2. Connected to a Computer with a Type C DP Port"></a>2. Connected to a Computer with a Type C DP Port</h2><p>Although smartphones are increasingly powerful, they still fall short of the computing might and electrical output of PCs. For a more robust AR setup, a computer with a Type C DP port is ideal for connecting to the XReal Air.</p><p>Users can opt to extend their display or mirror it. Extending ensures privacy, as only the wearer can see the content displayed. Meanwhile, mirroring is convenient for those who prefer not to switch between screens frequently. Encapsulated in the glasses, a state of deep focus is attainable, making it perfect for tackling urgent tasks.</p><p>This configuration is best suited for stationary environments such as offices, libraries, or coffee shops. It offers an additional, private screen that enhances productivity, especially in serene settings like a library.</p><h2 id="3-Utilizing-an-XReal-Beam"><a href="#3-Utilizing-an-XReal-Beam" class="headerlink" title="3. Utilizing an XReal Beam"></a>3. Utilizing an XReal Beam</h2><p>Connecting directly to a PC might not offer features like smooth follow or an anchored screen. Although the company’s Nebula software aims to bridge this gap, its beta versions for Windows and Mac struggle with consistency.</p><p>Enter the XReal Beam: a hardware solution that embodies the functionality of Nebula. It supports all viewing modes and comes with pre-installed apps, promising an elevated computer AR experience. Notably, it can connect to a Nintendo Switch, overcoming the console’s casting limitations through its dual Type C ports.</p><p>My preferred use case for the Beam is lounging on the couch, playing Switch games on what feels like the largest screen imaginable, all while enjoying unmatched comfort.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The XReal Air adapts to various setups, each offering distinct benefits tailored to specific lifestyles. Whether your daily routine aligns with any of these scenarios, the XReal Air is worthy of consideration for a step into augmented reality.</p><p>If you have any comments please share it in the <a href="https://github.com/hanlanyi/hanlanyi.github.io/issues">issues</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hi there! I’d love to hear your thoughts. Feel free to leave a &lt;a href=&quot;https://github.com/hanlanyi/hanlanyi.github.io/issues&quot;&gt;comment&lt;/a</summary>
      
    
    
    
    
  </entry>
  
</feed>
